{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions notebooks\n",
    "A notebook to test the implementation of the activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\KTorch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karim\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "from nn import GELU, ReLU, Sigmoid, Tanh, GLU\n",
    "from torch.nn import GELU as GELU_torch, ReLU as ReLU_torch, Sigmoid as Sigmoid_torch, Tanh as Tanh_torch, GLU as GLU_torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the input\n",
    "x = np.random.randn(2, 4, 8, 16)\n",
    "x = x.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor: [[[-0.145542    0.9518628   0.01008754  0.29289967  1.1653086\n",
      "   -0.13408564 -0.1687124   0.13282533]\n",
      "  [ 0.64741504  1.3694441  -0.15520082 -0.162142    0.02913002\n",
      "    1.5704583  -0.02686508 -0.02370054]\n",
      "  [-0.12090535 -0.00587273  1.4411118   0.5800386  -0.15547211\n",
      "   -0.03515472 -0.10513872 -0.0147208 ]\n",
      "  [-0.16978262 -0.16990755 -0.12638852 -0.1650315  -0.16536488\n",
      "    0.02318685 -0.01034079  0.09642731]]\n",
      "\n",
      " [[ 2.3479927   1.624473    0.681293    0.09421992 -0.15522046\n",
      "   -0.09950288  1.132856   -0.12115874]\n",
      "  [-0.04608486 -0.16790195  0.6228675  -0.12644741  1.0807629\n",
      "    1.2274061  -0.16919857 -0.12743708]\n",
      "  [-0.16996297  0.6696064   0.07631354 -0.0273305   0.17643358\n",
      "   -0.16586739 -0.09794919  2.0253866 ]\n",
      "  [-0.16257007 -0.10602092 -0.15754782  1.1833684  -0.16888998\n",
      "    2.628669    0.8891894  -0.01300707]]]\n",
      "GELU difference:  True\n",
      "ReLU difference:  True\n",
      "Sigmoid difference:  True\n",
      "Tanh difference:  True\n",
      "GLU difference:  True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from autograd.engine import Tensor\n",
    "# Defining the activation functions\n",
    "my_gelu, torch_gelu = GELU(), GELU_torch()\n",
    "my_relu, torch_relu = ReLU(), ReLU_torch()\n",
    "my_sigmoid, torch_sigmoid = Sigmoid(), Sigmoid_torch()\n",
    "my_tanh, torch_tanh = Tanh(), Tanh_torch()\n",
    "my_glu, torch_glu = GLU(), GLU_torch()\n",
    "\n",
    "# Getting the output\n",
    "my_gelu_y = my_gelu.forward(Tensor(x))\n",
    "torch_gelu_y = torch_gelu.forward(torch.tensor(x))\n",
    "my_relu_y = my_relu.forward(Tensor(x))\n",
    "torch_relu_y = torch_relu.forward(torch.tensor(x))\n",
    "my_sigmoid_y = my_sigmoid.forward(Tensor(x))\n",
    "torch_sigmoid_y = torch_sigmoid.forward(torch.tensor(x))\n",
    "my_tanh_y = my_tanh.forward(Tensor(x))\n",
    "torch_tanh_y = torch_tanh.forward(torch.tensor(x))\n",
    "my_glu_y = my_glu.forward(Tensor(x))\n",
    "torch_glu_y = torch_glu.forward(torch.tensor(x))\n",
    "\n",
    "# Checking the difference\n",
    "print(\"GELU difference: \", np.isclose(my_gelu_y.data, torch_gelu_y.cpu().detach().numpy(), atol=1e-8).all())\n",
    "print(\"ReLU difference: \", np.isclose(my_relu_y.data, torch_relu_y.cpu().detach().numpy(), atol=1e-8).all())\n",
    "print(\"Sigmoid difference: \", np.isclose(my_sigmoid_y.data, torch_sigmoid_y.cpu().detach().numpy(), atol=1e-8).all())\n",
    "print(\"Tanh difference: \", np.isclose(my_tanh_y.data, torch_tanh_y.cpu().detach().numpy(), atol=1e-8).all())\n",
    "print(\"GLU difference: \", np.isclose(my_glu_y.data, torch_glu_y.cpu().detach().numpy(), atol=1e-8).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, dtype('float32'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch_gelu_y.dtype), my_gelu_y.data.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back prop testing is in linear layer notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
