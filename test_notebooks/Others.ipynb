{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other examples\n",
    "A notebook to test simple layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/karimgamaleldin/projects/KTorch\n"
     ]
    }
   ],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from nn import Flatten, Linear, Dropout, Sequential, ReLU\n",
    "from core import KTorch\n",
    "from autograd import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6, 49152), 49152)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a tensor\n",
    "np.random.seed(0)\n",
    "x = np.random.randn(6, 128, 128, 3).astype(np.float32)\n",
    "x_tensor = Tensor(x)\n",
    "flatten = Flatten()\n",
    "flatten(x_tensor).shape, 128*128*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare output\n",
    "flattened = flatten(x_tensor)\n",
    "np_flattened = x.reshape(6, -1)\n",
    "np.equal(flattened.data, np_flattened).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input\n",
    "np.random.seed(0)\n",
    "x = np.random.randn(6, 128, 128, 3).astype(np.float32)\n",
    "x_tensor = Tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model\n",
    "flatten = Flatten()\n",
    "linear = Linear(128*128*3, 10)\n",
    "\n",
    "# Forward pass\n",
    "flattened = flatten(x_tensor)\n",
    "output = linear(flattened)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual forward pass\n",
    "\n",
    "# Parameters\n",
    "w = linear.weight.data\n",
    "b = linear.bias.data\n",
    "\n",
    "np_flattened = x.reshape(6, -1)\n",
    "manual_output = np_flattened @ w + b\n",
    "np.equal(output.data, manual_output).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual backward pass\n",
    "grad_output = np.ones_like(output.data)\n",
    "grad_flattened = grad_output @ w.T\n",
    "grad_flattened = grad_flattened.reshape(6, 128, 128, 3)\n",
    "grad_x = grad_flattened\n",
    "w_grad = np_flattened.T @ grad_output\n",
    "b_grad = grad_output.sum(axis=0)\n",
    "\n",
    "np.equal(x_tensor.grad.data, grad_x).all(), np.equal(linear.weight.grad.data, w_grad).all(), np.equal(linear.bias.grad.data, b_grad).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensor\n",
    "np.random.seed(0)\n",
    "x = np.random.randn(32, 128).astype(np.float32)\n",
    "x_tensor = Tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropout\n",
    "dropout = Dropout(0.5)\n",
    "linear = Linear(128, 10)\n",
    "\n",
    "output, mask = dropout(x_tensor)\n",
    "output = linear(output)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual forward propagation\n",
    "mask_np = mask.data\n",
    "w = linear.weight.data\n",
    "b = linear.bias.data\n",
    "\n",
    "\n",
    "masked = x * mask_np * (1.0 / 0.5)\n",
    "output_np = masked @ w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 10), True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_np.shape, np.equal(output.data, output_np).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual backward pass\n",
    "grad_output = np.ones_like(output.data)\n",
    "grad_masked = grad_output @ w.T\n",
    "grad_x = grad_masked * mask_np * (1.0 / 0.5)\n",
    "w_grad = masked.T @ grad_output\n",
    "b_grad = grad_output.sum(axis=0)\n",
    "\n",
    "np.equal(x_tensor.grad.data, grad_x).all(), np.equal(linear.weight.grad, w_grad).all(), np.equal(linear.bias.grad, b_grad).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get input\n",
    "np.random.seed(0)\n",
    "x = np.random.randn(32, 128).astype(np.float32)\n",
    "x_tensor = Tensor(x)\n",
    "\n",
    "# Create layers\n",
    "linear_1 = Linear(128, 64)\n",
    "relu = ReLU()\n",
    "linear_2 = Linear(64, 10)\n",
    "\n",
    "# Forward pass\n",
    "output = linear_1(x_tensor)\n",
    "output = relu(output)\n",
    "output = linear_2(output)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad values\n",
    "linear_2_weight_grad = linear_2.weight.grad\n",
    "linear_2_bias_grad = linear_2.bias.grad\n",
    "linear_1_weight_grad = linear_1.weight.grad\n",
    "linear_1_bias_grad = linear_1.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afdafasa\n",
      "afdafasa\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32, 10)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_2.weight.zero_grad()\n",
    "linear_2.bias.zero_grad()\n",
    "linear_1.weight.zero_grad()\n",
    "linear_1.bias.zero_grad()\n",
    "\n",
    "# Sequential forward pass\n",
    "seq = Sequential(linear_1, relu, linear_2)\n",
    "output_seq = seq(x_tensor)\n",
    "output_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.equal(output.data, output_seq.data).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True, True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_seq.backward()\n",
    "\n",
    "# Grad values\n",
    "np.equal(linear_2_weight_grad, linear_2.weight.grad).all(), np.equal(linear_2_bias_grad, linear_2.bias.grad).all(), np.equal(linear_1_weight_grad, linear_1.weight.grad).all(), np.equal(linear_1_bias_grad, linear_1.bias.grad).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
