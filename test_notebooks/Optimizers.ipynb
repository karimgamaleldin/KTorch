{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/karimgamaleldin/projects/KTorch\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "from nn import Sequential, Linear, Sigmoid, MSELoss, ReLU, BCELoss\n",
    "from optim import SGD, Adam, RMSProp, Adagrad, Adadelta\n",
    "import numpy as np\n",
    "from autograd import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a regression toy dataset\n",
    "np.random.seed(0)\n",
    "X_reg = np.random.randn(4096, 1)\n",
    "y_reg = 2*X_reg[:,0] + 1\n",
    "X_reg_tensor = Tensor(X_reg)\n",
    "y_reg_tensor = Tensor(y_reg)\n",
    "X_reg_test = np.random.randn(1024, 1)\n",
    "y_reg_test = 2*X_reg_test[:,0] + 1\n",
    "X_reg_test_tensor = Tensor(X_reg_test)\n",
    "y_reg_test_tensor = Tensor(y_reg_test)\n",
    "# Create a classification toy dataset\n",
    "np.random.seed(0)\n",
    "X_clf = np.random.randn(4096, 1)\n",
    "y_clf = (2*X_clf[:,0]) > 0\n",
    "X_clf_tensor = Tensor(X_clf)\n",
    "y_clf_tensor = Tensor(y_clf)\n",
    "\n",
    "X_clf_test = np.random.randn(1024, 1)\n",
    "y_clf_test = (2*X_clf_test[:,0]) > 0\n",
    "X_clf_test_tensor = Tensor(X_clf_test)\n",
    "y_clf_test_tensor = Tensor(y_clf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2096,), (2000,))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_clf_tensor.data[y_clf_tensor.data == 0].shape, y_clf_tensor.data[y_clf_tensor.data == 1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feed forward neural network for regression and classification\n",
    "regression_feed = Sequential(Linear(1, 16), ReLU(), Linear(16, 32), ReLU(), Linear(32, 16), ReLU(), Linear(16, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = MSELoss()\n",
    "optimizer = SGD(regression_feed.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor: 7.630028247833252"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = regression_feed(X_reg_test_tensor)\n",
    "loss = criterion(y_preds, y_reg_test_tensor)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 7.145706653594971, Test_Loss: 7.352579593658447\n",
      "Epoch 2/100, Loss: 6.88231897354126, Test_Loss: 7.106645107269287\n",
      "Epoch 3/100, Loss: 6.648864269256592, Test_Loss: 6.893398284912109\n",
      "Epoch 4/100, Loss: 6.44681453704834, Test_Loss: 6.710356712341309\n",
      "Epoch 5/100, Loss: 6.274351119995117, Test_Loss: 6.547443866729736\n",
      "Epoch 6/100, Loss: 6.121325969696045, Test_Loss: 6.39837646484375\n",
      "Epoch 7/100, Loss: 5.981585502624512, Test_Loss: 6.263370513916016\n",
      "Epoch 8/100, Loss: 5.855305194854736, Test_Loss: 6.141838073730469\n",
      "Epoch 9/100, Loss: 5.741837024688721, Test_Loss: 6.028887748718262\n",
      "Epoch 10/100, Loss: 5.63657283782959, Test_Loss: 5.923155784606934\n",
      "Epoch 11/100, Loss: 5.5382208824157715, Test_Loss: 5.823471546173096\n",
      "Epoch 12/100, Loss: 5.445682525634766, Test_Loss: 5.72920036315918\n",
      "Epoch 13/100, Loss: 5.358343124389648, Test_Loss: 5.6398539543151855\n",
      "Epoch 14/100, Loss: 5.275679111480713, Test_Loss: 5.554920673370361\n",
      "Epoch 15/100, Loss: 5.197242259979248, Test_Loss: 5.474076747894287\n",
      "Epoch 16/100, Loss: 5.1227240562438965, Test_Loss: 5.397025108337402\n",
      "Epoch 17/100, Loss: 5.051846981048584, Test_Loss: 5.323541641235352\n",
      "Epoch 18/100, Loss: 4.984385013580322, Test_Loss: 5.253452301025391\n",
      "Epoch 19/100, Loss: 4.920170783996582, Test_Loss: 5.1865315437316895\n",
      "Epoch 20/100, Loss: 4.858982086181641, Test_Loss: 5.1226325035095215\n",
      "Epoch 21/100, Loss: 4.800708770751953, Test_Loss: 5.0616021156311035\n",
      "Epoch 22/100, Loss: 4.745182514190674, Test_Loss: 5.003317832946777\n",
      "Epoch 23/100, Loss: 4.692297458648682, Test_Loss: 4.94766092300415\n",
      "Epoch 24/100, Loss: 4.641938209533691, Test_Loss: 4.894532203674316\n",
      "Epoch 25/100, Loss: 4.594019889831543, Test_Loss: 4.843839168548584\n",
      "Epoch 26/100, Loss: 4.548429489135742, Test_Loss: 4.795496940612793\n",
      "Epoch 27/100, Loss: 4.505117416381836, Test_Loss: 4.749431610107422\n",
      "Epoch 28/100, Loss: 4.463972091674805, Test_Loss: 4.705562114715576\n",
      "Epoch 29/100, Loss: 4.4249372482299805, Test_Loss: 4.663816928863525\n",
      "Epoch 30/100, Loss: 4.387922286987305, Test_Loss: 4.624131202697754\n",
      "Epoch 31/100, Loss: 4.352878093719482, Test_Loss: 4.586428165435791\n",
      "Epoch 32/100, Loss: 4.319723129272461, Test_Loss: 4.55064058303833\n",
      "Epoch 33/100, Loss: 4.288388252258301, Test_Loss: 4.516706943511963\n",
      "Epoch 34/100, Loss: 4.2588090896606445, Test_Loss: 4.484555721282959\n",
      "Epoch 35/100, Loss: 4.230921745300293, Test_Loss: 4.454126358032227\n",
      "Epoch 36/100, Loss: 4.204657077789307, Test_Loss: 4.425353527069092\n",
      "Epoch 37/100, Loss: 4.179941654205322, Test_Loss: 4.398175239562988\n",
      "Epoch 38/100, Loss: 4.156733989715576, Test_Loss: 4.37252950668335\n",
      "Epoch 39/100, Loss: 4.134960174560547, Test_Loss: 4.348355293273926\n",
      "Epoch 40/100, Loss: 4.114555835723877, Test_Loss: 4.325590133666992\n",
      "Epoch 41/100, Loss: 4.095456600189209, Test_Loss: 4.304176330566406\n",
      "Epoch 42/100, Loss: 4.077617645263672, Test_Loss: 4.28405237197876\n",
      "Epoch 43/100, Loss: 4.0609612464904785, Test_Loss: 4.265158653259277\n",
      "Epoch 44/100, Loss: 4.045437335968018, Test_Loss: 4.247438430786133\n",
      "Epoch 45/100, Loss: 4.030984878540039, Test_Loss: 4.230832576751709\n",
      "Epoch 46/100, Loss: 4.017554759979248, Test_Loss: 4.215287208557129\n",
      "Epoch 47/100, Loss: 4.005073547363281, Test_Loss: 4.200748443603516\n",
      "Epoch 48/100, Loss: 3.9935150146484375, Test_Loss: 4.18716287612915\n",
      "Epoch 49/100, Loss: 3.9828009605407715, Test_Loss: 4.174476146697998\n",
      "Epoch 50/100, Loss: 3.972904682159424, Test_Loss: 4.162644386291504\n",
      "Epoch 51/100, Loss: 3.9637558460235596, Test_Loss: 4.151614189147949\n",
      "Epoch 52/100, Loss: 3.9553170204162598, Test_Loss: 4.141340255737305\n",
      "Epoch 53/100, Loss: 3.947545289993286, Test_Loss: 4.131778717041016\n",
      "Epoch 54/100, Loss: 3.940392255783081, Test_Loss: 4.122884750366211\n",
      "Epoch 55/100, Loss: 3.933818817138672, Test_Loss: 4.114614963531494\n",
      "Epoch 56/100, Loss: 3.9277799129486084, Test_Loss: 4.106935501098633\n",
      "Epoch 57/100, Loss: 3.922246217727661, Test_Loss: 4.099804401397705\n",
      "Epoch 58/100, Loss: 3.917180299758911, Test_Loss: 4.093183517456055\n",
      "Epoch 59/100, Loss: 3.9125328063964844, Test_Loss: 4.087044715881348\n",
      "Epoch 60/100, Loss: 3.9082961082458496, Test_Loss: 4.081350803375244\n",
      "Epoch 61/100, Loss: 3.9044272899627686, Test_Loss: 4.076074123382568\n",
      "Epoch 62/100, Loss: 3.9008941650390625, Test_Loss: 4.071183681488037\n",
      "Epoch 63/100, Loss: 3.8976709842681885, Test_Loss: 4.066653251647949\n",
      "Epoch 64/100, Loss: 3.894745111465454, Test_Loss: 4.062457084655762\n",
      "Epoch 65/100, Loss: 3.8920822143554688, Test_Loss: 4.058570384979248\n",
      "Epoch 66/100, Loss: 3.889657735824585, Test_Loss: 4.054971218109131\n",
      "Epoch 67/100, Loss: 3.887457847595215, Test_Loss: 4.051639556884766\n",
      "Epoch 68/100, Loss: 3.885455846786499, Test_Loss: 4.048553943634033\n",
      "Epoch 69/100, Loss: 3.88364577293396, Test_Loss: 4.045697212219238\n",
      "Epoch 70/100, Loss: 3.8820080757141113, Test_Loss: 4.0430521965026855\n",
      "Epoch 71/100, Loss: 3.8805177211761475, Test_Loss: 4.040603160858154\n",
      "Epoch 72/100, Loss: 3.8791747093200684, Test_Loss: 4.038334369659424\n",
      "Epoch 73/100, Loss: 3.877962112426758, Test_Loss: 4.036231994628906\n",
      "Epoch 74/100, Loss: 3.8768625259399414, Test_Loss: 4.034285068511963\n",
      "Epoch 75/100, Loss: 3.8758718967437744, Test_Loss: 4.032480239868164\n",
      "Epoch 76/100, Loss: 3.8749754428863525, Test_Loss: 4.030807971954346\n",
      "Epoch 77/100, Loss: 3.8741631507873535, Test_Loss: 4.029257774353027\n",
      "Epoch 78/100, Loss: 3.873441457748413, Test_Loss: 4.027819633483887\n",
      "Epoch 79/100, Loss: 3.872783660888672, Test_Loss: 4.026484489440918\n",
      "Epoch 80/100, Loss: 3.872192144393921, Test_Loss: 4.025246620178223\n",
      "Epoch 81/100, Loss: 3.871657133102417, Test_Loss: 4.024096965789795\n",
      "Epoch 82/100, Loss: 3.8711764812469482, Test_Loss: 4.023029804229736\n",
      "Epoch 83/100, Loss: 3.8707470893859863, Test_Loss: 4.022037506103516\n",
      "Epoch 84/100, Loss: 3.8703598976135254, Test_Loss: 4.021115303039551\n",
      "Epoch 85/100, Loss: 3.870009422302246, Test_Loss: 4.020257949829102\n",
      "Epoch 86/100, Loss: 3.869701385498047, Test_Loss: 4.019459247589111\n",
      "Epoch 87/100, Loss: 3.8694167137145996, Test_Loss: 4.018716812133789\n",
      "Epoch 88/100, Loss: 3.8691654205322266, Test_Loss: 4.0180253982543945\n",
      "Epoch 89/100, Loss: 3.8689420223236084, Test_Loss: 4.017382621765137\n",
      "Epoch 90/100, Loss: 3.8687331676483154, Test_Loss: 4.016782760620117\n",
      "Epoch 91/100, Loss: 3.8685519695281982, Test_Loss: 4.01622200012207\n",
      "Epoch 92/100, Loss: 3.868389129638672, Test_Loss: 4.015700340270996\n",
      "Epoch 93/100, Loss: 3.8682398796081543, Test_Loss: 4.0152130126953125\n",
      "Epoch 94/100, Loss: 3.8681087493896484, Test_Loss: 4.014759540557861\n",
      "Epoch 95/100, Loss: 3.8679888248443604, Test_Loss: 4.014333248138428\n",
      "Epoch 96/100, Loss: 3.8678808212280273, Test_Loss: 4.013935565948486\n",
      "Epoch 97/100, Loss: 3.8677897453308105, Test_Loss: 4.013566493988037\n",
      "Epoch 98/100, Loss: 3.867706537246704, Test_Loss: 4.013219356536865\n",
      "Epoch 99/100, Loss: 3.8676350116729736, Test_Loss: 4.012894153594971\n",
      "Epoch 100/100, Loss: 3.867563009262085, Test_Loss: 4.0125908851623535\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "EPOCHS = 100\n",
    "for epoch in range(EPOCHS):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = regression_feed(X_reg_tensor)\n",
    "    loss = criterion(y_pred, y_reg_tensor)\n",
    "    loss.backward()\n",
    "    weights = [param.data for param in regression_feed.parameters()]\n",
    "    grads = [param.grad for param in regression_feed.parameters()]\n",
    "    optimizer.step()\n",
    "    updated_weights = [param.data for param in regression_feed.parameters()]\n",
    "    assert np.equal(weights[0] - 0.01*grads[0], updated_weights[0]).all()\n",
    "    y_preds_test = regression_feed(X_reg_test_tensor)\n",
    "    loss_test = criterion(y_preds_test, y_reg_test_tensor)\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}, Loss: {loss.data}, Test_Loss: {loss_test.data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification model\n",
    "np.random.seed(0)\n",
    "classification_feed = Sequential(Linear(1, 16), ReLU(), Linear(16, 32), ReLU(), Linear(32, 16), ReLU(), Linear(16, 1), Sigmoid())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = BCELoss()\n",
    "optimizer = SGD(classification_feed.parameters(), lr=0.2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.498046875"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean((y_true.data == (y_pred.data > 0.5)).astype(int))\n",
    "\n",
    "acc = accuracy(y_clf_test_tensor, classification_feed(X_clf_test_tensor))\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.6942612528800964, Test_Accuracy: 0.498046875 Train_Accuracy: 0.51171875\n",
      "Epoch 2/5, Loss: 0.6934086084365845, Test_Accuracy: 0.498046875 Train_Accuracy: 0.5117073059082031\n",
      "Epoch 3/5, Loss: 0.6928922533988953, Test_Accuracy: 0.5012779235839844 Train_Accuracy: 0.49240684509277344\n",
      "Epoch 4/5, Loss: 0.6932503581047058, Test_Accuracy: 0.5018081665039062 Train_Accuracy: 0.48914527893066406\n",
      "Epoch 5/5, Loss: 0.6937583684921265, Test_Accuracy: 0.5017814636230469 Train_Accuracy: 0.4893455505371094\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = classification_feed(X_clf_tensor)\n",
    "    loss = criterion(y_pred, y_clf_tensor)\n",
    "    loss.backward()\n",
    "    weights = [param.data for param in classification_feed.parameters()]\n",
    "    grads = [param.grad for param in classification_feed.parameters()]\n",
    "    prev_velocities = optimizer.prev_velocities[0]\n",
    "    momentum = optimizer.momentum\n",
    "    dampening = optimizer.dampening\n",
    "    optimizer.step()\n",
    "    updated_weights = [param.data for param in classification_feed.parameters()]\n",
    "    assert np.equal(weights[0] - 0.2*(momentum * prev_velocities + (1 - dampening) * grads[0]), updated_weights[0]).all()\n",
    "    y_preds_test = classification_feed(X_clf_test_tensor)\n",
    "    acc = accuracy(y_clf_test_tensor, y_preds_test)\n",
    "    y_preds_train = classification_feed(X_clf_tensor)\n",
    "    acc_train = accuracy(y_clf_tensor, y_preds_train)\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}, Loss: {loss.data}, Test_Accuracy: {acc}', f'Train_Accuracy: {acc_train}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reg model\n",
    "np.random.seed(0)\n",
    "regression_feed = Sequential(Linear(1, 16), ReLU(), Linear(16, 32), ReLU(), Linear(32, 16), ReLU(), Linear(16, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
