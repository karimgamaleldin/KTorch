from core.BaseEstimator import BaseEstimator
from metrics.ClassificationMetrics import accuracy_score
import numpy as np

class LinearDiscriminantAnalysis(BaseEstimator):

  '''
  Linear Discriminant Analysis (LDA)

  sklearn: A classifer with linear decision boundary, generated by fitting class conditional densities to the data and using Bayes' rule.
  '''
  def __init__(self, priors=None):
    super().__init__('LDA', 'Discriminant Analysis', accuracy_score)
    self.priors = priors

  
  def fit(self, X, y):
    if not isinstance(X, np.ndarray):
      X = np.asarray(X)
    if not isinstance(y, np.ndarray):
      y = np.asarray(y)

    if self.priors is not None:
      self.priors = np.asarray(self.priors)
    else:
      self.priors = np.bincount(y) / float(len(y)) # Estimating the probability of each class from the data

    self.classes_ = np.unique(y) # Unique classes in the target
    n_features, n_classes = X.shape[1], len(self.classes_) # Number of features and classes
    self.means_ = np.zeros((n_classes, n_features)) # Mean of each feature per class
    self.covariance_ = np.zeros((n_features, n_features)) # Covariance matrix of each feature

    for i, y_i in enumerate(self.classes_):
      X_i = X[y == y_i] # Selecting the data for the class
      n_i = len(X_i) # Number of samples in the class
      self.means_[i] = X_i.mean(axis=0) # Mean of the class
      self.covariance_ += self.priors[i] * np.cov(X_i, bias=True, rowvar=False) * (n_i - 1) # Covariance matrix of the class, a weighted sum of the covariance of the class

    self.covariance_ /= (len(X) - n_classes) # Covariance matrix of the data

    print('Linear Discrimninant Analysis model has been trained.')

  def predict(self, X):
    '''
    Shapes:
      - X: (n_samples, n_features)
      - means_: (n_classes, n_features)
      - covariance_: (n_features, n_features)
      - priors: (n_classes)
    '''
    if not isinstance(X, np.ndarray):
      X = np.asarray(X)

    temp = np.dot(np.linalg.inv(self.covariance_), self.means_.T) # (n_features, n_features) * (n_features, n_classes) = (n_features, n_classes)
    term1 = np.dot(X, temp) # (n_samples, n_features) * (n_features, n_classes) = (n_samples, n_classes)
    term2 = -0.5 * np.sum(self.means_ * temp.T, axis=1) + np.log(self.priors)  # (n_classes,)
    decision = term1 + term2
    return self.classes_[np.argmax(decision, axis=1)]  # Predicting the class with the highest decision value

  # def predict(self, X):
  #   if not isinstance(X, np.ndarray):
  #     X = np.asarray(X)

  #   temp = np.dot(np.linalg.inv(self.covariance_), self.means_.T) # (n_features, n_features) * (n_features, n_classes) = (n_features, n_classes
  #   decision = np.dot(X, temp) - 0.5 * np.dot(self.means_, temp) + np.log(self.priors) # (n_samples, n_features) * (n_features, n_classes) = (n_samples, n_classes)
  #   return self.classes_[np.argmax(decision, axis=1)]  # Predicting the class with the highest decision value

  def evaluate(self, X, y, metric):
    preds = self.predict(X)
    return metric(y, preds)

  def score(self, X, y):
    return self.evaluate(X, y, self._base_metric)
  

