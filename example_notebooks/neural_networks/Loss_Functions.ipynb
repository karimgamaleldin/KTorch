{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/karimgamaleldin\n"
     ]
    }
   ],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "from nn import BCELoss, CrossEntropyLoss, MSELoss, BCEWithLogitsLoss, Linear, ReLU, Sigmoid, Softmax\n",
    "from core import KTorch\n",
    "from autograd import Tensor\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get inputs\n",
    "np.random.seed(0)\n",
    "\n",
    "y_preds = np.random.randint(100, 1000, (32, ))\n",
    "y_true = np.random.randint(100, 1000, (32, ))\n",
    "y_preds == y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum Loss: tensor: 3806477.0\n",
      "Mean Loss: tensor: 118952.40625\n"
     ]
    }
   ],
   "source": [
    "y_preds_tensor = Tensor(y_preds)\n",
    "y_true_tensor = Tensor(y_true)\n",
    "\n",
    "# MSE Loss\n",
    "mse_loss_sum = MSELoss(reduction='sum')\n",
    "mse_loss_mean = MSELoss(reduction='mean')\n",
    "\n",
    "# Calculate loss\n",
    "loss_sum = mse_loss_sum(y_preds_tensor, y_true_tensor)\n",
    "loss_mean = mse_loss_mean(y_preds_tensor, y_true_tensor)\n",
    "\n",
    "print(f\"Sum Loss: {loss_sum}\")\n",
    "print(f\"Mean Loss: {loss_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Sum Loss: 3806477\n",
      "Manual Mean Loss: 118952.40625\n"
     ]
    }
   ],
   "source": [
    "# Calculate mse loss manual\n",
    "diff = y_preds - y_true\n",
    "mse_mean = np.mean(diff**2)\n",
    "mse_sum = np.sum(diff**2)\n",
    "\n",
    "print(f\"Manual Sum Loss: {mse_sum}\")\n",
    "print(f\"Manual Mean Loss: {mse_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing backprop\n",
    "\n",
    "# input \n",
    "np.random.seed(0)\n",
    "input = np.random.randn(32, 10).astype(np.float32)\n",
    "target = np.random.rand(32, 1).astype(np.float32)\n",
    "\n",
    "# Convert to tensors\n",
    "x = Tensor(input)\n",
    "y = Tensor(target)\n",
    "\n",
    "# model\n",
    "linear_1 = Linear(10, 5)\n",
    "relu = ReLU()\n",
    "linear_2 = Linear(5, 1)\n",
    "relu_2 = ReLU()\n",
    "\n",
    "# forward pass\n",
    "h1_nn = linear_1(x)\n",
    "h2_nn = relu(h1_nn)\n",
    "h3_nn = linear_2(h2_nn)\n",
    "out = relu_2(h3_nn)\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual forward pass\n",
    "weights_1 = linear_1.weight.data\n",
    "bias_1 = linear_1.bias.data\n",
    "weights_2 = linear_2.weight.data\n",
    "bias_2 = linear_2.bias.data\n",
    "\n",
    "h1 = np.dot(input, weights_1) + bias_1\n",
    "h2 = np.maximum(h1, 0)\n",
    "h3 = np.dot(h2, weights_2) + bias_2\n",
    "out_manual = np.maximum(h3, 0)\n",
    "\n",
    "out_manual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.equal(out.data, out_manual).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss\n",
    "loss = MSELoss(reduction='mean')\n",
    "loss_val = loss(out, y)\n",
    "\n",
    "# Manual loss\n",
    "diff = out_manual - target\n",
    "loss_manual = np.mean(diff**2)\n",
    "\n",
    "np.equal(loss_val.data, loss_manual).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "loss_val.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Manual backward pass\n",
    "dL = np.ones_like(diff)\n",
    "print((loss_val.grad == dL).all())\n",
    "d_diff = dL * 2 * diff / 32\n",
    "d_out_manual = d_diff\n",
    "print((d_out_manual == out.grad).all())\n",
    "d_h3 = d_out_manual * (h3 > 0)\n",
    "print((d_h3 == h3_nn.grad).all())\n",
    "d_weights_2 = np.dot(h2.T, d_h3)\n",
    "d_bias_2 = np.sum(d_h3, axis=0)\n",
    "print((d_weights_2 == linear_2.weight.grad).all())\n",
    "print((d_bias_2 == linear_2.bias.grad).all())\n",
    "d_h2 = np.dot(d_h3, weights_2.T)\n",
    "print((d_h2 == h2_nn.grad).all())\n",
    "d_h1 = d_h2 * (h1 > 0)\n",
    "print((d_h1 == h1_nn.grad).all())\n",
    "d_weights_1 = np.dot(input.T, d_h1)\n",
    "d_bias_1 = np.sum(d_h1, axis=0)\n",
    "print((d_weights_1 == linear_1.weight.grad).all())\n",
    "print((d_bias_1 == linear_1.bias.grad).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSE Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing backprop\n",
    "\n",
    "# input \n",
    "np.random.seed(0)\n",
    "input = np.random.randn(32, 10).astype(np.float32)\n",
    "target = np.random.rand(32, 1).astype(np.float32)\n",
    "\n",
    "# Convert to tensors\n",
    "x = Tensor(input)\n",
    "y = Tensor(target)\n",
    "\n",
    "# model\n",
    "linear_1 = Linear(10, 5)\n",
    "relu = ReLU()\n",
    "linear_2 = Linear(5, 1)\n",
    "relu_2 = ReLU()\n",
    "\n",
    "# forward pass\n",
    "h1_nn = linear_1(x)\n",
    "h2_nn = relu(h1_nn)\n",
    "h3_nn = linear_2(h2_nn)\n",
    "out = relu_2(h3_nn)\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual forward pass\n",
    "weights_1 = linear_1.weight.data\n",
    "bias_1 = linear_1.bias.data\n",
    "weights_2 = linear_2.weight.data\n",
    "bias_2 = linear_2.bias.data\n",
    "\n",
    "h1 = np.dot(input, weights_1) + bias_1\n",
    "h2 = np.maximum(h1, 0)\n",
    "h3 = np.dot(h2, weights_2) + bias_2\n",
    "out_manual = np.maximum(h3, 0)\n",
    "\n",
    "out_manual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.equal(out.data, out_manual).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss\n",
    "loss = MSELoss(reduction='sum')\n",
    "loss_val = loss(out, y)\n",
    "\n",
    "# Manual loss\n",
    "diff = out_manual - target\n",
    "loss_manual = np.sum(diff**2)\n",
    "\n",
    "np.equal(loss_val.data, loss_manual).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "loss_val.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Manual backward pass\n",
    "dL = np.ones_like(diff)\n",
    "print((loss_val.grad == dL).all())\n",
    "d_diff = dL * 2 * diff\n",
    "d_out_manual = d_diff\n",
    "print((d_out_manual == out.grad).all())\n",
    "d_h3 = d_out_manual * (h3 > 0)\n",
    "print((d_h3 == h3_nn.grad).all())\n",
    "d_weights_2 = np.dot(h2.T, d_h3)\n",
    "d_bias_2 = np.sum(d_h3, axis=0)\n",
    "print((d_weights_2 == linear_2.weight.grad).all())\n",
    "print((d_bias_2 == linear_2.bias.grad).all())\n",
    "d_h2 = np.dot(d_h3, weights_2.T)\n",
    "print((d_h2 == h2_nn.grad).all())\n",
    "d_h1 = d_h2 * (h1 > 0)\n",
    "print((d_h1 == h1_nn.grad).all())\n",
    "d_weights_1 = np.dot(input.T, d_h1)\n",
    "d_bias_1 = np.sum(d_h1, axis=0)\n",
    "print((d_weights_1 == linear_1.weight.grad).all())\n",
    "print((d_bias_1 == linear_1.bias.grad).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BCELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Loss: tensor: 1.1908915042877197\n",
      "Sum Loss: tensor: 38.10852813720703\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get inputs\n",
    "np.random.seed(0)\n",
    "inputs = np.random.randn(32, 10).astype(np.float32)\n",
    "targets = np.random.randint(0, 2, (32, 1)).astype(np.float32)\n",
    "\n",
    "# Convert to tensors\n",
    "x = Tensor(inputs)\n",
    "y = Tensor(targets)\n",
    "\n",
    "# Try forward pass of bce loss\n",
    "preds = np.random.rand(32, 1).astype(np.float32)\n",
    "preds_tensor = Tensor(preds)\n",
    "\n",
    "bce_loss_mean = BCELoss(reduction='mean')\n",
    "bce_loss_sum = BCELoss(reduction='sum')\n",
    "\n",
    "loss_mean = bce_loss_mean(preds_tensor, y)\n",
    "loss_sum = bce_loss_sum(preds_tensor, y)\n",
    "\n",
    "print(f\"Mean Loss: {loss_mean}\")\n",
    "print(f\"Sum Loss: {loss_sum}\")\n",
    "\n",
    "loss_mean.data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Mean Loss: 1.1908915042877197\n",
      "Manual Sum Loss: 38.10852813720703\n"
     ]
    }
   ],
   "source": [
    "# Manual loss calculation\n",
    "log_1 = np.log(preds).astype(np.float32)\n",
    "log_2 = np.log(1 - preds).astype(np.float32)\n",
    "log_1 = np.clip(log_1, -100, float('inf'))\n",
    "log_2 = np.clip(log_2, -100, float('inf'))\n",
    "loss_term_1 = - targets * log_1\n",
    "loss_term_2 = - (1 - targets) * log_2\n",
    "loss = loss_term_1 + loss_term_2\n",
    "loss_manual_mean = np.mean(loss)\n",
    "loss_manual_sum = np.sum(loss)\n",
    "\n",
    "print(f\"Manual Mean Loss: {loss_manual_mean}\")\n",
    "print(f\"Manual Sum Loss: {loss_manual_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check loss values\n",
    "np.equal(loss_mean.data, loss_manual_mean), np.equal(loss_sum.data, loss_manual_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass\n",
    "np.random.seed(0)\n",
    "\n",
    "layer_1 = Linear(10, 5)\n",
    "relu = ReLU()\n",
    "layer_2 = Linear(5, 1)\n",
    "sigmoid = Sigmoid()\n",
    "\n",
    "# Forward pass\n",
    "h1 = layer_1(x)\n",
    "h2 = relu(h1)\n",
    "h3 = layer_2(h2)\n",
    "out = sigmoid(h3)\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual forward pass\n",
    "weights_1 = layer_1.weight.data\n",
    "bias_1 = layer_1.bias.data\n",
    "weights_2 = layer_2.weight.data\n",
    "bias_2 = layer_2.bias.data\n",
    "\n",
    "h1_manual = np.dot(inputs, weights_1) + bias_1\n",
    "h2_manual = np.maximum(h1_manual, 0)\n",
    "h3_manual = np.dot(h2_manual, weights_2) + bias_2\n",
    "out_manual = 1 / (1 + np.exp(-h3_manual))\n",
    "\n",
    "out_manual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the outputs are the same\n",
    "np.equal(out.data, out_manual).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss\n",
    "bce_loss = BCELoss(reduction='mean')\n",
    "loss_val = bce_loss(out, y)\n",
    "\n",
    "# Manual loss\n",
    "log_1 = np.log(out_manual).astype(np.float32)\n",
    "sub_out_manual = 1 - out_manual\n",
    "log_2 = np.log(sub_out_manual).astype(np.float32)\n",
    "log_1_clip = np.clip(log_1, -100, float('inf'))\n",
    "log_2_clip = np.clip(log_2, -100, float('inf'))\n",
    "loss_term_1 = - targets * log_1_clip\n",
    "loss_term_2 = - (1 - targets) * log_2_clip\n",
    "loss = loss_term_1 + loss_term_2\n",
    "loss_manual = np.mean(loss)\n",
    "\n",
    "np.equal(loss_val.data, loss_manual).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "loss_val.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True  *  * clamp(log(sigmoid(ReLU( @  + ) @  + ))) +  *  +  *  * clamp(log(sigmoid(ReLU( @  + ) @  + ) *  + ))\n",
      "True\n",
      "True\n",
      "True clamp(log(sigmoid(ReLU( @  + ) @  + )))\n",
      "True clamp(log(sigmoid(ReLU( @  + ) @  + ) *  + ))\n",
      "True log(sigmoid(ReLU( @  + ) @  + ))\n",
      "True log(sigmoid(ReLU( @  + ) @  + ) *  + )\n",
      "True sigmoid(ReLU( @  + ) @  + )\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Manual backward pass\n",
    "dL = np.ones_like(loss)\n",
    "print((loss_val.grad == dL).all())\n",
    "\n",
    "d_loss = dL / 32\n",
    "print((d_loss == loss_val._prev[0].grad).all(), loss_val._prev[0].label)\n",
    "\n",
    "d_loss_term_1 = d_loss\n",
    "print((d_loss_term_1 == loss_val._prev[0]._prev[0].grad).all()) \n",
    "\n",
    "d_loss_term_2 = d_loss\n",
    "print((d_loss_term_1 == loss_val._prev[0]._prev[1].grad).all()) \n",
    "\n",
    "d_log_1_clip = - targets * d_loss_term_1\n",
    "print((d_log_1_clip == loss_val._prev[0]._prev[0]._prev[1].grad).all(), loss_val._prev[0]._prev[0]._prev[1].label) \n",
    "\n",
    "d_log_2_clip = d_loss_term_2 * - (1 - targets)\n",
    "print((d_log_2_clip == loss_val._prev[0]._prev[1]._prev[1].grad).all(), loss_val._prev[0]._prev[1]._prev[1].label)\n",
    "\n",
    "d_log_1  = d_log_1_clip * (log_1 > -100)\n",
    "print((d_log_1 == loss_val._prev[0]._prev[0]._prev[1]._prev[0].grad).all(), loss_val._prev[0]._prev[0]._prev[1]._prev[0].label)\n",
    "\n",
    "d_log_2 = d_log_2_clip * (log_2 > -100)\n",
    "print((d_log_2 == loss_val._prev[0]._prev[1]._prev[1]._prev[0].grad).all(), loss_val._prev[0]._prev[1]._prev[1]._prev[0].label)\n",
    "\n",
    "d_out_manual = d_log_1  / out_manual\n",
    "d_sub_out_manual = d_log_2 / sub_out_manual\n",
    "d_out_manual = d_out_manual - d_sub_out_manual\n",
    "print((d_out_manual == out.grad).all(), out.label)\n",
    "\n",
    "d_h3_manual = d_out_manual * out_manual * (1 - out_manual)  \n",
    "print((d_h3_manual == h3.grad).all())\n",
    "\n",
    "d_weights_2 = np.dot(h2_manual.T, d_h3_manual)\n",
    "d_bias_2 = np.sum(d_h3_manual, axis=0)\n",
    "print((d_weights_2 == layer_2.weight.grad).all())\n",
    "\n",
    "d_h2_manual = np.dot(d_h3_manual, weights_2.T)\n",
    "print((d_h2_manual == h2.grad).all())\n",
    "\n",
    "d_h1_manual = d_h2_manual * (h1_manual > 0)\n",
    "print((d_h1_manual == h1.grad).all())\n",
    "\n",
    "d_weights_1 = np.dot(inputs.T, d_h1_manual)\n",
    "d_bias_1 = np.sum(d_h1_manual, axis=0)\n",
    "print((d_weights_1 == layer_1.weight.grad).all())\n",
    "print((d_bias_1 == layer_1.bias.grad).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Loss: tensor: 33.56257629394531\n",
      "Sum Loss: tensor: 1074.00244140625\n"
     ]
    }
   ],
   "source": [
    "# Get inputs\n",
    "np.random.seed(0)\n",
    "inputs = np.random.randn(32, 10).astype(np.float32)\n",
    "targets = np.random.randint(0, 2, (32, 1)).astype(np.float32)\n",
    "\n",
    "# Convert to tensors\n",
    "x = Tensor(inputs)\n",
    "y = Tensor(targets)\n",
    "\n",
    "# Try forward pass of bce loss\n",
    "preds = np.random.randint(-100, 100, (32, 1)).astype(np.float32)\n",
    "preds_tensor = Tensor(preds)\n",
    "\n",
    "bce_logit_loss_mean = BCEWithLogitsLoss(reduction='mean')\n",
    "bce_logit_loss_sum = BCEWithLogitsLoss(reduction='sum')\n",
    "\n",
    "loss_mean = bce_logit_loss_mean(preds_tensor, y)\n",
    "loss_sum = bce_logit_loss_sum(preds_tensor, y)\n",
    "\n",
    "print(f\"Mean Loss: {loss_mean}\")\n",
    "print(f\"Sum Loss: {loss_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Mean Loss: 33.56257629394531\n",
      "Manual Sum Loss: 1074.00244140625\n"
     ]
    }
   ],
   "source": [
    "# Manual loss calculation\n",
    "sigmoid_output = 1 / (1 + np.exp(-preds))\n",
    "log_1 = np.log(sigmoid_output).astype(np.float32)\n",
    "log_2 = np.log(1 - sigmoid_output).astype(np.float32)\n",
    "log_1 = np.clip(log_1, -100, float('inf'))\n",
    "log_2 = np.clip(log_2, -100, float('inf'))\n",
    "loss_term_1 = - targets * log_1\n",
    "loss_term_2 = - (1 - targets) * log_2\n",
    "loss = loss_term_1 + loss_term_2\n",
    "loss_manual_mean = np.mean(loss)\n",
    "loss_manual_sum = np.sum(loss)\n",
    "\n",
    "print(f\"Manual Mean Loss: {loss_manual_mean}\")\n",
    "print(f\"Manual Sum Loss: {loss_manual_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check loss values\n",
    "np.equal(loss_mean.data, loss_manual_mean), np.equal(loss_sum.data, loss_manual_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass\n",
    "np.random.seed(0)\n",
    "\n",
    "layer_1 = Linear(10, 5)\n",
    "relu = ReLU()\n",
    "layer_2 = Linear(5, 1)\n",
    "\n",
    "# Forward pass\n",
    "h1 = layer_1(x)\n",
    "h2 = relu(h1)\n",
    "out = layer_2(h2)\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual forward pass\n",
    "weights_1 = layer_1.weight.data\n",
    "bias_1 = layer_1.bias.data\n",
    "weights_2 = layer_2.weight.data\n",
    "bias_2 = layer_2.bias.data\n",
    "\n",
    "h1_manual = np.dot(inputs, weights_1) + bias_1\n",
    "h2_manual = np.maximum(h1_manual, 0)\n",
    "h3_manual = np.dot(h2_manual, weights_2) + bias_2\n",
    "out_manual = 1 / (1 + np.exp(-h3_manual))\n",
    "\n",
    "out_manual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the outputs are the same\n",
    "np.equal(Sigmoid()(out).data, out_manual).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss\n",
    "bce_loss = BCEWithLogitsLoss(reduction='mean')\n",
    "loss_val = bce_loss(out, y)\n",
    "\n",
    "# Manual loss\n",
    "log_1 = np.log(out_manual).astype(np.float32)\n",
    "sub_out_manual = 1 - out_manual\n",
    "log_2 = np.log(sub_out_manual).astype(np.float32)\n",
    "log_1_clip = np.clip(log_1, -100, float('inf'))\n",
    "log_2_clip = np.clip(log_2, -100, float('inf'))\n",
    "loss_term_1 = - targets * log_1_clip\n",
    "loss_term_2 = - (1 - targets) * log_2_clip\n",
    "loss = loss_term_1 + loss_term_2\n",
    "loss_manual = np.mean(loss)\n",
    "\n",
    "np.equal(loss_val.data, loss_manual).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "loss_val.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True  *  * clamp(log(sigmoid(ReLU( @  + ) @  + ))) +  *  +  *  * clamp(log(sigmoid(ReLU( @  + ) @  + ) *  + ))\n",
      "True\n",
      "True\n",
      "True clamp(log(sigmoid(ReLU( @  + ) @  + )))\n",
      "True clamp(log(sigmoid(ReLU( @  + ) @  + ) *  + ))\n",
      "True log(sigmoid(ReLU( @  + ) @  + ))\n",
      "True log(sigmoid(ReLU( @  + ) @  + ) *  + )\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Manual backward pass\n",
    "dL = np.ones_like(loss)\n",
    "print((loss_val.grad == dL).all())\n",
    "\n",
    "d_loss = dL / 32\n",
    "print((d_loss == loss_val._prev[0].grad).all(), loss_val._prev[0].label)\n",
    "\n",
    "d_loss_term_1 = d_loss\n",
    "print((d_loss_term_1 == loss_val._prev[0]._prev[0].grad).all()) \n",
    "\n",
    "d_loss_term_2 = d_loss\n",
    "print((d_loss_term_1 == loss_val._prev[0]._prev[1].grad).all()) \n",
    "\n",
    "d_log_1_clip = - targets * d_loss_term_1\n",
    "print((d_log_1_clip == loss_val._prev[0]._prev[0]._prev[1].grad).all(), loss_val._prev[0]._prev[0]._prev[1].label) \n",
    "\n",
    "d_log_2_clip = d_loss_term_2 * - (1 - targets)\n",
    "print((d_log_2_clip == loss_val._prev[0]._prev[1]._prev[1].grad).all(), loss_val._prev[0]._prev[1]._prev[1].label)\n",
    "\n",
    "d_log_1  = d_log_1_clip * (log_1 > -100)\n",
    "print((d_log_1 == loss_val._prev[0]._prev[0]._prev[1]._prev[0].grad).all(), loss_val._prev[0]._prev[0]._prev[1]._prev[0].label)\n",
    "\n",
    "d_log_2 = d_log_2_clip * (log_2 > -100)\n",
    "print((d_log_2 == loss_val._prev[0]._prev[1]._prev[1]._prev[0].grad).all(), loss_val._prev[0]._prev[1]._prev[1]._prev[0].label)\n",
    "\n",
    "d_out_manual = d_log_1  / out_manual\n",
    "d_sub_out_manual = d_log_2 / sub_out_manual\n",
    "d_out_manual = d_out_manual - d_sub_out_manual\n",
    "\n",
    "d_h3_manual = d_out_manual * out_manual * (1 - out_manual)  \n",
    "print((d_h3_manual == out.grad).all())\n",
    "\n",
    "d_weights_2 = np.dot(h2_manual.T, d_h3_manual)\n",
    "d_bias_2 = np.sum(d_h3_manual, axis=0)\n",
    "print((d_weights_2 == layer_2.weight.grad).all())\n",
    "\n",
    "d_h2_manual = np.dot(d_h3_manual, weights_2.T)\n",
    "print((d_h2_manual == h2.grad).all())\n",
    "\n",
    "d_h1_manual = d_h2_manual * (h1_manual > 0)\n",
    "print((d_h1_manual == h1.grad).all())\n",
    "\n",
    "d_weights_1 = np.dot(inputs.T, d_h1_manual)\n",
    "d_bias_1 = np.sum(d_h1_manual, axis=0)\n",
    "print((d_weights_1 == layer_1.weight.grad).all())\n",
    "print((d_bias_1 == layer_1.bias.grad).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CrossEntropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
